'''
å¯¹æ¯”è¯•éªŒ
6.é‡æ–°è®¾è®¡ç¼“å­˜ç­–ç•¥
å¯¹æ¯”å®éªŒï¼šå¸¦ç¼“å­˜ç­–ç•¥çš„VQAé—­ç¯ç³»ç»Ÿ
åœ¨å®¢æˆ·ç«¯å®ç°å¤šçº§ç¼“å­˜ï¼Œå‡å°‘é‡å¤è®¡ç®—
'''


import os
import json
import time
import csv
import numpy as np
from PIL import Image
import requests
from dataclasses import dataclass, asdict
from typing import List, Dict, Tuple, Any, Optional
from tqdm import tqdm
import re
import base64
from io import BytesIO
import hashlib
import pickle
from functools import lru_cache
from concurrent.futures import ThreadPoolExecutor, as_completed


# ==================== é…ç½® ====================
@dataclass
class Config:
    # æœåŠ¡å™¨é…ç½®
    SERVER_IP = "è¿™æ˜¯æˆ‘çš„æœåŠ¡å™¨IPåœ°å€ï¼Œæˆ‘éšè—äº†"
    QWEN_URL = f"http://{SERVER_IP}:8020/chat_vl"
    CLIP_URL = f"http://{SERVER_IP}:8021/clip/score"
    SAM_URL = f"http://{SERVER_IP}:8022/segment_by_bbox"

    # æ•°æ®é›†è·¯å¾„
    DATA_ROOT = r"C:\Users\kuanzhang\Desktop\courseB\fuwuqisanhaoji\MyVQA\combined_dataset"
    METADATA_PATH = os.path.join(DATA_ROOT, "combined_metadata.json")
    IMAGE_DIR = os.path.join(DATA_ROOT, "images")

    # å®éªŒå‚æ•°
    MAX_RETRIES = 2
    CONFIDENCE_THRESHOLD = 0.2
    TEMP_EVIDENCE_PATH = "./temp_evidence.png"

    # ç¼“å­˜é…ç½®
    ENABLE_CACHE = True  # æ˜¯å¦å¯ç”¨ç¼“å­˜
    CACHE_DIR = "./cache"  # ç¼“å­˜ç›®å½•
    IMAGE_CACHE_DIR = os.path.join(CACHE_DIR, "images")
    TEXT_CACHE_DIR = os.path.join(CACHE_DIR, "text")
    RESULT_CACHE_DIR = os.path.join(CACHE_DIR, "results")
    SAM_CACHE_DIR = os.path.join(CACHE_DIR, "sam")

    # ç¼“å­˜ç­–ç•¥å‚æ•°
    CACHE_WARMUP_SIZE = 20  # é¢„çƒ­ç¼“å­˜æ ·æœ¬æ•°
    USE_SIMILARITY_CACHE = True  # æ˜¯å¦ä½¿ç”¨ç›¸ä¼¼æ€§ç¼“å­˜
    SIMILARITY_THRESHOLD = 0.95  # ç›¸ä¼¼æ€§é˜ˆå€¼
    BATCH_SIZE = 4  # æ‰¹é‡å¤„ç†å¤§å°ï¼ˆå¹¶è¡Œï¼‰

    # è¾“å‡ºè·¯å¾„
    OUTPUT_DIR = "./results_with_cache"
    RESULTS_JSON = os.path.join(OUTPUT_DIR, "results_with_cache.json")
    STATS_CSV = os.path.join(OUTPUT_DIR, "statistics_with_cache.csv")
    SAM_SEGMENTS_DIR = os.path.join(OUTPUT_DIR, "sam_segments")
    CACHE_STATS_FILE = os.path.join(OUTPUT_DIR, "cache_statistics.json")

    # å®éªŒè®¾ç½®
    NUM_SAMPLES = 110
    RANDOM_SEED = 42


# ==================== ç¼“å­˜ç®¡ç†å™¨ ====================
class CacheManager:
    """å¤šçº§ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, config: Config):
        self.config = config
        self.cache_stats = {
            "image_hits": 0,
            "image_misses": 0,
            "text_hits": 0,
            "text_misses": 0,
            "sam_hits": 0,
            "sam_misses": 0,
            "result_hits": 0,
            "result_misses": 0,
            "similarity_hits": 0,
            "warmup_hits": 0,
            "total_requests": 0
        }

        # åˆ›å»ºç¼“å­˜ç›®å½•
        if config.ENABLE_CACHE:
            for dir_path in [config.IMAGE_CACHE_DIR, config.TEXT_CACHE_DIR,
                             config.RESULT_CACHE_DIR, config.SAM_CACHE_DIR]:
                os.makedirs(dir_path, exist_ok=True)

        # å†…å­˜ç¼“å­˜ï¼ˆLRUç¼“å­˜ï¼‰
        self.image_hash_cache = {}
        self.text_hash_cache = {}
        self.sam_cache = {}
        self.result_cache = {}

        # ç›¸ä¼¼æ€§ç¼“å­˜ç´¢å¼•
        self.similarity_index = {}

        print(f"ğŸ”„ åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨ï¼Œå¯ç”¨ç¼“å­˜: {config.ENABLE_CACHE}")

    def get_image_hash(self, image_path: str) -> str:
        """è·å–å›¾åƒå“ˆå¸Œï¼ˆç”¨äºç¼“å­˜é”®ï¼‰"""
        if image_path in self.image_hash_cache:
            return self.image_hash_cache[image_path]

        try:
            with Image.open(image_path) as img:
                # ä½¿ç”¨ç¼©ç•¥å›¾è®¡ç®—å“ˆå¸Œï¼Œæé«˜é€Ÿåº¦
                img.thumbnail((128, 128))
                img_gray = img.convert('L')
                pixels = list(img_gray.getdata())
                avg = sum(pixels) / len(pixels)
                hash_str = ''.join(['1' if pixel > avg else '0' for pixel in pixels])
                hash_value = hashlib.md5(hash_str.encode()).hexdigest()

                self.image_hash_cache[image_path] = hash_value
                return hash_value
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è®¡ç®—å›¾åƒå“ˆå¸Œ: {e}")
            return hashlib.md5(image_path.encode()).hexdigest()

    def get_text_hash(self, text: str) -> str:
        """è·å–æ–‡æœ¬å“ˆå¸Œï¼ˆç”¨äºç¼“å­˜é”®ï¼‰"""
        if text in self.text_hash_cache:
            return self.text_hash_cache[text]

        hash_value = hashlib.md5(text.encode('utf-8')).hexdigest()
        self.text_hash_cache[text] = hash_value
        return hash_value

    def get_sam_cache_key(self, image_path: str, bbox_str: str) -> str:
        """è·å–SAMç¼“å­˜é”®"""
        image_hash = self.get_image_hash(image_path)
        bbox_hash = hashlib.md5(bbox_str.encode()).hexdigest()
        return f"{image_hash}_{bbox_hash}"

    def get_result_cache_key(self, image_path: str, question: str) -> str:
        """è·å–ç»“æœç¼“å­˜é”®"""
        image_hash = self.get_image_hash(image_path)
        question_hash = self.get_text_hash(question)
        return f"{image_hash}_{question_hash}"

    def check_image_cache(self, image_path: str) -> Optional[bytes]:
        """æ£€æŸ¥å›¾åƒç¼“å­˜"""
        self.cache_stats["total_requests"] += 1
        if not self.config.ENABLE_CACHE:
            return None

        image_hash = self.get_image_hash(image_path)
        cache_path = os.path.join(self.config.IMAGE_CACHE_DIR, f"{image_hash}.pkl")

        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    self.cache_stats["image_hits"] += 1
                    return pickle.load(f)
            except:
                pass

        self.cache_stats["image_misses"] += 1
        return None

    def save_image_cache(self, image_path: str, image_data: bytes):
        """ä¿å­˜å›¾åƒç¼“å­˜"""
        if not self.config.ENABLE_CACHE:
            return

        image_hash = self.get_image_hash(image_path)
        cache_path = os.path.join(self.config.IMAGE_CACHE_DIR, f"{image_hash}.pkl")

        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(image_data, f)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å›¾åƒç¼“å­˜å¤±è´¥: {e}")

    def check_text_cache(self, text: str) -> Optional[Any]:
        """æ£€æŸ¥æ–‡æœ¬ç¼“å­˜"""
        self.cache_stats["total_requests"] += 1
        if not self.config.ENABLE_CACHE:
            return None

        text_hash = self.get_text_hash(text)
        cache_path = os.path.join(self.config.TEXT_CACHE_DIR, f"{text_hash}.pkl")

        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    self.cache_stats["text_hits"] += 1
                    return pickle.load(f)
            except:
                pass

        self.cache_stats["text_misses"] += 1
        return None

    def save_text_cache(self, text: str, data: Any):
        """ä¿å­˜æ–‡æœ¬ç¼“å­˜"""
        if not self.config.ENABLE_CACHE:
            return

        text_hash = self.get_text_hash(text)
        cache_path = os.path.join(self.config.TEXT_CACHE_DIR, f"{text_hash}.pkl")

        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ–‡æœ¬ç¼“å­˜å¤±è´¥: {e}")

    def check_sam_cache(self, image_path: str, bbox_str: str) -> Optional[bytes]:
        """æ£€æŸ¥SAMç¼“å­˜"""
        self.cache_stats["total_requests"] += 1
        if not self.config.ENABLE_CACHE:
            return None

        cache_key = self.get_sam_cache_key(image_path, bbox_str)

        # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
        if cache_key in self.sam_cache:
            self.cache_stats["sam_hits"] += 1
            return self.sam_cache[cache_key]

        # æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
        cache_path = os.path.join(self.config.SAM_CACHE_DIR, f"{cache_key}.pkl")

        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    segment_data = pickle.load(f)
                    self.sam_cache[cache_key] = segment_data
                    self.cache_stats["sam_hits"] += 1
                    return segment_data
            except:
                pass

        self.cache_stats["sam_misses"] += 1
        return None

    def save_sam_cache(self, image_path: str, bbox_str: str, segment_data: bytes):
        """ä¿å­˜SAMç¼“å­˜"""
        if not self.config.ENABLE_CACHE:
            return

        cache_key = self.get_sam_cache_key(image_path, bbox_str)
        cache_path = os.path.join(self.config.SAM_CACHE_DIR, f"{cache_key}.pkl")

        try:
            # ä¿å­˜åˆ°å†…å­˜ç¼“å­˜
            self.sam_cache[cache_key] = segment_data

            # ä¿å­˜åˆ°æ–‡ä»¶ç¼“å­˜
            with open(cache_path, 'wb') as f:
                pickle.dump(segment_data, f)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜SAMç¼“å­˜å¤±è´¥: {e}")

    def check_result_cache(self, image_path: str, question: str) -> Optional[Dict]:
        """æ£€æŸ¥ç»“æœç¼“å­˜"""
        self.cache_stats["total_requests"] += 1
        if not self.config.ENABLE_CACHE:
            return None

        cache_key = self.get_result_cache_key(image_path, question)

        # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
        if cache_key in self.result_cache:
            self.cache_stats["result_hits"] += 1
            return self.result_cache[cache_key]

        # æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
        cache_path = os.path.join(self.config.RESULT_CACHE_DIR, f"{cache_key}.json")

        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'r', encoding='utf-8') as f:
                    result = json.load(f)
                    self.result_cache[cache_key] = result
                    self.cache_stats["result_hits"] += 1
                    return result
            except:
                pass

        self.cache_stats["result_misses"] += 1
        return None

    def save_result_cache(self, image_path: str, question: str, result: Dict):
        """ä¿å­˜ç»“æœç¼“å­˜"""
        if not self.config.ENABLE_CACHE:
            return

        cache_key = self.get_result_cache_key(image_path, question)
        cache_path = os.path.join(self.config.RESULT_CACHE_DIR, f"{cache_key}.json")

        try:
            # ä¿å­˜åˆ°å†…å­˜ç¼“å­˜
            self.result_cache[cache_key] = result

            # ä¿å­˜åˆ°æ–‡ä»¶ç¼“å­˜
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç»“æœç¼“å­˜å¤±è´¥: {e}")

    def find_similar_cached_result(self, image_path: str, question: str) -> Optional[Dict]:
        """æŸ¥æ‰¾ç›¸ä¼¼ç¼“å­˜ç»“æœï¼ˆåŸºäºç›¸ä¼¼æ€§ï¼‰"""
        if not self.config.ENABLE_CACHE or not self.config.USE_SIMILARITY_CACHE:
            return None

        # ç®€å•çš„ç›¸ä¼¼æ€§åŒ¹é…ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒå›¾åƒä½†ä¸åŒé—®é¢˜çš„ç¼“å­˜
        image_hash = self.get_image_hash(image_path)

        # æŸ¥æ‰¾ç›¸åŒå›¾åƒçš„ç¼“å­˜
        if image_hash in self.similarity_index:
            for cached_question, cached_result in self.similarity_index[image_hash].items():
                # ç®€å•çš„é—®é¢˜ç›¸ä¼¼æ€§æ£€æŸ¥ï¼ˆå…±äº«å…³é”®è¯ï¼‰
                question_words = set(question.lower().split())
                cached_words = set(cached_question.lower().split())
                common_words = question_words.intersection(cached_words)

                if len(common_words) / max(len(question_words), 1) > 0.5:
                    self.cache_stats["similarity_hits"] += 1
                    return cached_result

        return None

    def update_similarity_index(self, image_path: str, question: str, result: Dict):
        """æ›´æ–°ç›¸ä¼¼æ€§ç´¢å¼•"""
        if not self.config.ENABLE_CACHE or not self.config.USE_SIMILARITY_CACHE:
            return

        image_hash = self.get_image_hash(image_path)

        if image_hash not in self.similarity_index:
            self.similarity_index[image_hash] = {}

        self.similarity_index[image_hash][question] = result

    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_hits = (self.cache_stats["image_hits"] +
                      self.cache_stats["text_hits"] +
                      self.cache_stats["sam_hits"] +
                      self.cache_stats["result_hits"] +
                      self.cache_stats["similarity_hits"])

        total_misses = (self.cache_stats["image_misses"] +
                        self.cache_stats["text_misses"] +
                        self.cache_stats["sam_misses"] +
                        self.cache_stats["result_misses"])

        hit_rate = total_hits / (total_hits + total_misses) if (total_hits + total_misses) > 0 else 0

        stats = self.cache_stats.copy()
        stats.update({
            "total_hits": total_hits,
            "total_misses": total_misses,
            "hit_rate": hit_rate,
            "memory_cache_size": {
                "image_hashes": len(self.image_hash_cache),
                "text_hashes": len(self.text_hash_cache),
                "sam_cache": len(self.sam_cache),
                "result_cache": len(self.result_cache)
            }
        })

        return stats

    def print_cache_stats(self):
        """æ‰“å°ç¼“å­˜ç»Ÿè®¡"""
        stats = self.get_cache_stats()
        print(f"\nğŸ“Š ç¼“å­˜ç»Ÿè®¡:")
        print(f"   æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
        print(f"   å›¾åƒç¼“å­˜å‘½ä¸­ç‡: {stats['image_hits']}/{stats['image_hits'] + stats['image_misses']} "
              f"({stats['image_hits'] / (stats['image_hits'] + stats['image_misses']) * 100 if (stats['image_hits'] + stats['image_misses']) > 0 else 0:.1f}%)")
        print(f"   æ–‡æœ¬ç¼“å­˜å‘½ä¸­ç‡: {stats['text_hits']}/{stats['text_hits'] + stats['text_misses']} "
              f"({stats['text_hits'] / (stats['text_hits'] + stats['text_misses']) * 100 if (stats['text_hits'] + stats['text_misses']) > 0 else 0:.1f}%)")
        print(f"   SAMç¼“å­˜å‘½ä¸­ç‡: {stats['sam_hits']}/{stats['sam_hits'] + stats['sam_misses']} "
              f"({stats['sam_hits'] / (stats['sam_hits'] + stats['sam_misses']) * 100 if (stats['sam_hits'] + stats['sam_misses']) > 0 else 0:.1f}%)")
        print(f"   ç»“æœç¼“å­˜å‘½ä¸­ç‡: {stats['result_hits']}/{stats['result_hits'] + stats['result_misses']} "
              f"({stats['result_hits'] / (stats['result_hits'] + stats['result_misses']) * 100 if (stats['result_hits'] + stats['result_misses']) > 0 else 0:.1f}%)")
        print(f"   ç›¸ä¼¼æ€§ç¼“å­˜å‘½ä¸­: {stats['similarity_hits']}")
        print(f"   æ€»ä½“å‘½ä¸­ç‡: {stats['hit_rate'] * 100:.1f}%")


# ==================== æ•°æ®ç»“æ„ ====================
@dataclass
class ExperimentResult:
    sample_id: int
    image_file: str
    question: str
    ground_truth_answers: List[str]

    # ç³»ç»Ÿè¾“å‡º
    initial_answer: str = ""
    initial_bbox: str = ""
    refined_answer: str = ""
    final_confidence: float = 0.0
    clip_scores: Dict[str, float] = None

    # æ€§èƒ½æŒ‡æ ‡
    iteration_count: int = 0
    sam_calls: int = 0
    clip_calls: int = 0
    qwen_calls: int = 0
    total_time: float = 0.0

    # ç¼“å­˜ç›¸å…³æŒ‡æ ‡
    cache_hits: Dict[str, int] = None  # è®°å½•å„ç±»ç¼“å­˜å‘½ä¸­
    from_cache: bool = False  # æ˜¯å¦æ¥è‡ªç¼“å­˜

    # è¯„ä¼°
    accuracy: float = 0.0
    is_correct: bool = False
    failure_type: str = ""
    notes: str = ""

    def __post_init__(self):
        if self.clip_scores is None:
            self.clip_scores = {}
        if self.cache_hits is None:
            self.cache_hits = {
                "image": 0,
                "text": 0,
                "sam": 0,
                "result": 0,
                "similarity": 0
            }


@dataclass
class SystemStatistics:
    total_samples: int = 0
    correct_samples: int = 0
    total_iterations: int = 0
    total_sam_calls: int = 0
    total_clip_calls: int = 0
    total_qwen_calls: int = 0
    total_time: float = 0.0

    # ç¼“å­˜ç»Ÿè®¡
    cache_stats: Dict[str, Any] = None

    # åŠ é€Ÿæ¯”ç»Ÿè®¡
    estimated_speedup: float = 0.0
    cache_benefit_ratio: float = 0.0

    # æŒ‰å¤±è´¥ç±»å‹ç»Ÿè®¡
    failure_counts: Dict[str, int] = None

    def __post_init__(self):
        if self.failure_counts is None:
            self.failure_counts = {
                "location_failure": 0,
                "segmentation_failure": 0,
                "reasoning_failure": 0,
                "verification_failure": 0,
                "other": 0
            }
        if self.cache_stats is None:
            self.cache_stats = {}

    @property
    def accuracy(self) -> float:
        return self.correct_samples / self.total_samples if self.total_samples > 0 else 0

    @property
    def avg_iterations(self) -> float:
        return self.total_iterations / self.total_samples if self.total_samples > 0 else 0

    @property
    def avg_time_per_sample(self) -> float:
        return self.total_time / self.total_samples if self.total_samples > 0 else 0

    @property
    def avg_qwen_calls(self) -> float:
        return self.total_qwen_calls / self.total_samples if self.total_samples > 0 else 0

    @property
    def avg_sam_calls(self) -> float:
        return self.total_sam_calls / self.total_samples if self.total_samples > 0 else 0

    @property
    def avg_clip_calls(self) -> float:
        return self.total_clip_calls / self.total_samples if self.total_samples > 0 else 0


# ==================== å·¥å…·å‡½æ•°ï¼ˆå¸¦ç¼“å­˜ï¼‰ ====================
def load_textvqa_dataset(config: Config) -> List[Dict]:
    """åŠ è½½myVQAæ•°æ®é›†"""
    with open(config.METADATA_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # éšæœºé€‰æ‹©æ ·æœ¬ï¼ˆç¡®ä¿å¯å¤ç°ï¼‰
    np.random.seed(config.RANDOM_SEED)
    selected_indices = np.random.choice(len(data), min(config.NUM_SAMPLES, len(data)), replace=False)

    samples = []
    for idx in selected_indices:
        sample = data[idx]
        sample['id'] = idx
        samples.append(sample)

    print(f"ğŸ“Š åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")
    return samples


def image_to_base64(image_path: str, max_size=(512, 512), cache_manager: CacheManager = None) -> str:
    """å›¾åƒè½¬Base64ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    # æ£€æŸ¥å›¾åƒç¼“å­˜
    if cache_manager:
        cached_data = cache_manager.check_image_cache(image_path)
        if cached_data:
            return cached_data

    try:
        img = Image.open(image_path)
        if img.mode in ("RGBA", "P"):
            img = img.convert("RGB")
        img.thumbnail(max_size)
        buffered = BytesIO()
        img.save(buffered, format="JPEG", quality=85)
        image_data = base64.b64encode(buffered.getvalue()).decode('utf-8')

        # ä¿å­˜åˆ°ç¼“å­˜
        if cache_manager:
            cache_manager.save_image_cache(image_path, image_data)

        return image_data
    except Exception as e:
        print(f"âŒ å›¾åƒè½¬Base64å¤±è´¥: {e}")
        return ""


def call_qwen(prompt: str, image_path: str = None, config: Config = None,
              cache_manager: CacheManager = None, use_cache: bool = True) -> str:
    """è°ƒç”¨Qwen-VLæœåŠ¡ï¼ˆå¸¦ç¼“å­˜ï¼‰"""

    # æ„å»ºç¼“å­˜é”®
    cache_key = None
    if cache_manager and use_cache and image_path:
        cache_key = cache_manager.get_result_cache_key(image_path, prompt)
        cached_result = cache_manager.check_result_cache(image_path, prompt)
        if cached_result and 'response' in cached_result:
            print(f"ğŸ’¾ Qwenç»“æœæ¥è‡ªç¼“å­˜")
            return cached_result['response']

    try:
        payload = {"prompt": prompt}
        if image_path and os.path.exists(image_path):
            print(f"ğŸ“¤ å‘é€å›¾åƒ: {os.path.basename(image_path)}")
            payload["image_url"] = image_to_base64(image_path, cache_manager=cache_manager)

        response = requests.post(config.QWEN_URL, json=payload, timeout=120)

        print(f"ğŸ“¡ Qwenå“åº”çŠ¶æ€: {response.status_code}")

        if response.status_code == 200:
            res = response.json()
            print(f"ğŸ“¥ QwenåŸå§‹å“åº”: {res}")
            response_text = res.get("response", "").strip()

            # ä¿å­˜åˆ°ç¼“å­˜
            if cache_manager and cache_key and use_cache:
                cache_manager.save_result_cache(image_path, prompt, {
                    'response': response_text,
                    'timestamp': time.time()
                })

            return response_text
        else:
            print(f"âŒ Qwenè°ƒç”¨å¤±è´¥: HTTP {response.status_code} - {response.text}")
    except requests.exceptions.Timeout:
        print("â° Qwenè°ƒç”¨è¶…æ—¶")
    except Exception as e:
        print(f"ğŸ’¥ Qwenè°ƒç”¨å¼‚å¸¸: {type(e).__name__}: {e}")
    return ""


def call_sam(image_path: str, bbox_str: str, config: Config,
             save_segment: bool = True, iteration: int = 1,
             cache_manager: CacheManager = None) -> Tuple[bool, bytes]:
    """è°ƒç”¨SAMæœåŠ¡ï¼ˆå¸¦ç¼“å­˜ï¼‰"""

    # æ£€æŸ¥SAMç¼“å­˜
    if cache_manager:
        cached_segment = cache_manager.check_sam_cache(image_path, bbox_str)
        if cached_segment:
            print(f"ğŸ’¾ SAMç»“æœæ¥è‡ªç¼“å­˜")
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            with open(config.TEMP_EVIDENCE_PATH, "wb") as out:
                out.write(cached_segment)

            # å¦‚æœéœ€è¦ä¿å­˜åˆ†å‰²å›¾åƒ
            if save_segment:
                segment_path = save_sam_segment(
                    cached_segment, image_path, bbox_str, iteration, config
                )
                print(f"ğŸ’¾ SAMåˆ†å‰²å›¾åƒå·²ä¿å­˜: {segment_path}")

            return True, cached_segment

    try:
        with open(image_path, 'rb') as f:
            files = {'image': f}
            data = {'bbox': bbox_str}
            response = requests.post(config.SAM_URL, files=files, data=data, timeout=30)

            if response.status_code == 200:
                segment_data = response.content

                # ä¿å­˜åˆ°ç¼“å­˜
                if cache_manager:
                    cache_manager.save_sam_cache(image_path, bbox_str, segment_data)

                # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ç”¨äºåç»­å¤„ç†
                with open(config.TEMP_EVIDENCE_PATH, "wb") as out:
                    out.write(segment_data)

                # å¦‚æœéœ€è¦ä¿å­˜åˆ†å‰²å›¾åƒ
                if save_segment:
                    segment_path = save_sam_segment(
                        segment_data, image_path, bbox_str, iteration, config
                    )
                    print(f"ğŸ’¾ SAMåˆ†å‰²å›¾åƒå·²ä¿å­˜: {segment_path}")

                return True, segment_data
            else:
                print(f"âŒ SAMè°ƒç”¨å¤±è´¥: HTTP {response.status_code} - {response.text}")
    except Exception as e:
        print(f"ğŸ’¥ SAMè°ƒç”¨å¼‚å¸¸: {type(e).__name__}: {e}")
    return False, None


def call_clip(image_bytes: bytes, text_label: str, config: Config,
              cache_manager: CacheManager = None) -> float:
    """è°ƒç”¨CLIPæœåŠ¡ï¼Œè¿”å›æœ€é«˜ç›¸ä¼¼åº¦ï¼ˆå¸¦ç¼“å­˜ï¼‰"""

    # æ„å»ºç¼“å­˜é”®ï¼ˆå›¾åƒå“ˆå¸Œ + æ–‡æœ¬å“ˆå¸Œï¼‰
    if cache_manager:
        # è®¡ç®—å›¾åƒå“ˆå¸Œ
        image_hash = hashlib.md5(image_bytes).hexdigest()[:16]
        text_hash = cache_manager.get_text_hash(text_label)
        cache_key = f"{image_hash}_{text_hash}"

        cached_score = cache_manager.check_text_cache(cache_key)
        if cached_score is not None:
            print(f"ğŸ’¾ CLIPç»“æœæ¥è‡ªç¼“å­˜: {cached_score:.3f}")
            return float(cached_score)

    files = {'imagefile': ('evidence.png', image_bytes, 'image/png')}
    data = {'text': text_label, 'temperature': 100.0}

    try:
        response = requests.post(config.CLIP_URL, files=files, data=data, timeout=10)
        if response.status_code == 200:
            res = response.json()
            if res.get('results'):
                # è¿”å›æ‰€æœ‰æ ‡ç­¾ä¸­çš„æœ€é«˜ç›¸ä¼¼åº¦
                similarities = [v['similarity'] for v in res['results'].values()]
                max_similarity = float(max(similarities)) if similarities else 0.0

                # ä¿å­˜åˆ°ç¼“å­˜
                if cache_manager:
                    cache_manager.save_text_cache(f"{image_hash}_{text_hash}", max_similarity)

                return max_similarity
        else:
            print(f"âŒ CLIPè°ƒç”¨å¤±è´¥: HTTP {response.status_code} - {response.text}")
    except Exception as e:
        print(f"ğŸ’¥ CLIPè°ƒç”¨å¼‚å¸¸: {type(e).__name__}: {e}")
    return 0.0


def extract_bbox_from_text(text: str, img_w: int, img_h: int) -> str:
    """ä»æ–‡æœ¬ä¸­æå–bboxåæ ‡"""
    patterns = [
        r'\((\d+)\s*[,ï¼Œ]\s*(\d+)\)\s*\((\d+)\s*[,ï¼Œ]\s*(\d+)\)',
        r'(\d+)\s*[,ï¼Œ]\s*(\d+)\s+(\d+)\s*[,ï¼Œ]\s*(\d+)',
        r'(\d+)\s*[,ï¼Œ]\s*(\d+)\s*[,ï¼Œ]\s*(\d+)\s*[,ï¼Œ]\s*(\d+)',
        r'åæ ‡[ï¼š:]?\s*\(?(\d+)\s*[,ï¼Œ]\s*(\d+)\)?\s*\(?(\d+)\s*[,ï¼Œ]\s*(\d+)\)?',
    ]

    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            x1, y1, x2, y2 = map(int, match.groups())
            # ç¡®ä¿åæ ‡åœ¨å›¾åƒèŒƒå›´å†…
            x1, x2 = sorted([max(0, min(img_w, x)) for x in (x1, x2)])
            y1, y2 = sorted([max(0, min(img_h, y)) for y in (y1, y2)])
            return f"{x1},{y1},{x2},{y2}"

    # æœªæ‰¾åˆ°åæ ‡ï¼Œè¿”å›å…¨å›¾
    return f"0,0,{img_w},{img_h}"


def normalize_answer(answer: str) -> str:
    """æ ‡å‡†åŒ–ç­”æ¡ˆï¼šå°å†™ã€ç§»é™¤æ ‡ç‚¹ã€ç©ºæ ¼"""
    if not answer:
        return ""
    # è½¬æ¢ä¸ºå°å†™
    answer = answer.lower()
    # ç§»é™¤æ ‡ç‚¹ç¬¦å·
    answer = re.sub(r'[^\w\s]', '', answer)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    answer = ' '.join(answer.split())
    return answer


def calculate_accuracy(predicted_answer: str, ground_truths: List[str]) -> Tuple[float, bool]:
    """è®¡ç®—ç­”æ¡ˆå‡†ç¡®æ€§ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    if not predicted_answer:
        return 0.0, False

    pred_normalized = normalize_answer(predicted_answer)

    for truth in ground_truths:
        if not truth:
            continue

        truth_normalized = normalize_answer(truth)

        # ç²¾ç¡®åŒ¹é…
        if pred_normalized == truth_normalized:
            return 1.0, True

        # åŒ…å«åŒ¹é…ï¼ˆé’ˆå¯¹è¾ƒé•¿ç­”æ¡ˆï¼‰
        if truth_normalized in pred_normalized or pred_normalized in truth_normalized:
            return 1.0, True

        # æ•°å­—æå–åŒ¹é…ï¼ˆé’ˆå¯¹OCRé—®é¢˜ï¼‰
        pred_digits = ''.join(filter(str.isdigit, pred_normalized))
        truth_digits = ''.join(filter(str.isdigit, truth_normalized))
        if pred_digits and pred_digits == truth_digits:
            return 1.0, True

        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®å“ç‰Œ/åç§°
        common_brands = ['yamaha', 'red', 'mike lee', 'aj52uyv']
        for brand in common_brands:
            if brand in pred_normalized and brand in truth_normalized:
                return 1.0, True

    return 0.0, False


def analyze_failure_type(result: ExperimentResult, config: Config) -> str:
    """åˆ†æå¤±è´¥ç±»å‹"""
    if result.final_confidence < config.CONFIDENCE_THRESHOLD:
        return "verification_failure"
    elif result.iteration_count == 0:
        return "location_failure"
    elif "æ— æ³•" in result.refined_answer or "ä¸èƒ½" in result.refined_answer:
        return "reasoning_failure"
    else:
        return "other"


# ==================== æ‰¹é‡å¤„ç†å·¥å…· ====================
def batch_call_qwen(prompts_with_images: List[Tuple[str, str]], config: Config,
                    cache_manager: CacheManager = None) -> List[str]:
    """æ‰¹é‡è°ƒç”¨Qwen-VLæœåŠ¡"""
    results = []

    # å¦‚æœæ²¡æœ‰å¯ç”¨æ‰¹é‡æˆ–æ‰¹é‡å¤§å°ä¸º1ï¼Œåˆ™é¡ºåºå¤„ç†
    if config.BATCH_SIZE <= 1 or len(prompts_with_images) <= 1:
        for prompt, image_path in prompts_with_images:
            result = call_qwen(prompt, image_path, config, cache_manager)
            results.append(result)
        return results

    # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡Œå¤„ç†
    with ThreadPoolExecutor(max_workers=min(config.BATCH_SIZE, len(prompts_with_images))) as executor:
        future_to_index = {}

        for i, (prompt, image_path) in enumerate(prompts_with_images):
            future = executor.submit(call_qwen, prompt, image_path, config, cache_manager)
            future_to_index[future] = i

        # åˆå§‹åŒ–ç»“æœåˆ—è¡¨
        results = [None] * len(prompts_with_images)

        # è·å–ç»“æœ
        for future in as_completed(future_to_index):
            index = future_to_index[future]
            try:
                results[index] = future.result()
            except Exception as e:
                print(f"âŒ æ‰¹é‡Qwenè°ƒç”¨å¤±è´¥ (ç´¢å¼•{index}): {e}")
                results[index] = ""

    return results


# ==================== ä¸»å®éªŒæµç¨‹ï¼ˆå¸¦ç¼“å­˜ï¼‰ ====================
def run_single_experiment(sample: Dict, config: Config,
                          cache_manager: CacheManager = None,
                          warmup_mode: bool = False) -> ExperimentResult:
    """è¿è¡Œå•ä¸ªæ ·æœ¬çš„å®éªŒï¼ˆå¸¦ç¼“å­˜ç‰ˆæœ¬ï¼‰"""
    result = ExperimentResult(
        sample_id=sample['id'],
        image_file=sample['image_file'],
        question=sample['question'],
        ground_truth_answers=sample['answers']
    )

    start_time = time.time()
    image_path = os.path.join(config.IMAGE_DIR, sample['image_file'])

    # Step 1: æ£€æŸ¥å®Œæ•´ç»“æœç¼“å­˜
    if cache_manager and not warmup_mode:
        cached_result = cache_manager.check_result_cache(image_path, sample['question'])
        if cached_result and 'full_result' in cached_result:
            print(f"ğŸ’¾ å®Œæ•´ç»“æœæ¥è‡ªç¼“å­˜")
            result.from_cache = True
            result.initial_answer = cached_result['full_result'].get('initial_answer', '')
            result.initial_bbox = cached_result['full_result'].get('initial_bbox', '')
            result.refined_answer = cached_result['full_result'].get('refined_answer', '')
            result.final_confidence = cached_result['full_result'].get('final_confidence', 0.0)
            result.iteration_count = cached_result['full_result'].get('iteration_count', 0)
            result.sam_calls = 0  # æ¥è‡ªç¼“å­˜ï¼Œæ²¡æœ‰å®é™…è°ƒç”¨
            result.clip_calls = 0
            result.qwen_calls = 0

            # è¯„ä¼°å‡†ç¡®æ€§
            answer_to_evaluate = result.refined_answer if result.refined_answer else result.initial_answer
            result.accuracy, result.is_correct = calculate_accuracy(
                answer_to_evaluate,
                sample['answers']
            )

            result.total_time = time.time() - start_time
            return result

    # Step 2: è·å–å›¾åƒå°ºå¯¸
    try:
        with Image.open(image_path) as img:
            img_w, img_h = img.size
    except Exception as e:
        result.notes = f"æ— æ³•æ‰“å¼€å›¾åƒ: {e}"
        result.total_time = time.time() - start_time
        return result

    # Step 3: Qwenåˆæ­¥å›ç­” + å®šä½
    prompt1 = f"é—®é¢˜ï¼š{sample['question']} è¯·å…ˆç»™å‡ºç­”æ¡ˆï¼›å†ä»¥æ ¼å¼(å·¦ä¸Šè§’xåæ ‡,å·¦ä¸Šè§’yåæ ‡) (å³ä¸‹è§’xåæ ‡,å³ä¸‹è§’yåæ ‡) ä¸¤ç‚¹ç”Ÿæˆçš„çŸ©å½¢æ¡†å°†å›¾ç‰‡éœ€è¦å…³æ³¨åŒºåŸŸåŒ…å›´è¿›å»ã€‚"
    print(f"ğŸ“¤ å‘é€ç»™Qwençš„æç¤º: {prompt1}")

    initial_response = call_qwen(prompt1, image_path, config, cache_manager, use_cache=not warmup_mode)
    result.qwen_calls += 1

    if not initial_response:
        result.notes = "Qwenåˆæ­¥å›ç­”å¤±è´¥"
        result.total_time = time.time() - start_time
        return result

    print(f"ğŸ“¥ Qwenåˆæ­¥å›ç­”: {initial_response}")
    result.initial_answer = initial_response
    result.initial_bbox = extract_bbox_from_text(initial_response, img_w, img_h)
    print(f"ğŸ“ æå–çš„BBox: {result.initial_bbox}")

    # Step 4: é—­ç¯éªŒè¯å¾ªç¯
    bbox_str = result.initial_bbox
    refined_answer = ""
    confidence = 0.0
    iteration = 0

    for retry in range(config.MAX_RETRIES + 1):
        iteration += 1
        print(f"ğŸ”„ ç¬¬ {iteration} æ¬¡è¿­ä»£å°è¯•...")

        # è°ƒç”¨SAMåˆ†å‰²ï¼Œå¹¶ä¿å­˜å›¾åƒï¼ˆå¸¦ç¼“å­˜ï¼‰
        success, segment_data = call_sam(image_path, bbox_str, config,
                                         save_segment=True, iteration=iteration,
                                         cache_manager=cache_manager)
        if not success:
            result.notes = f"SAMåˆ†å‰²å¤±è´¥ (è¿­ä»£{iteration})"
            break

        result.sam_calls += 1

        # æ£€æŸ¥è¯æ®å›¾æ˜¯å¦å­˜åœ¨
        if not os.path.exists(config.TEMP_EVIDENCE_PATH):
            result.notes = f"è¯æ®å›¾æœªç”Ÿæˆ (è¿­ä»£{iteration})"
            break

        # è¯»å–è¯æ®å›¾
        try:
            evidence_size = os.path.getsize(config.TEMP_EVIDENCE_PATH)
            if evidence_size == 0:
                result.notes = f"è¯æ®å›¾ä¸ºç©ºæ–‡ä»¶ (è¿­ä»£{iteration})"
                break

            with open(config.TEMP_EVIDENCE_PATH, "rb") as f:
                evidence_bytes = f.read()
        except Exception as e:
            result.notes = f"è¯»å–è¯æ®å›¾å¤±è´¥: {e}"
            break

        # QwenåŸºäºè¯æ®å›¾é‡æ–°å›ç­”
        prompt2 = f"åªçœ‹è¿™å¼ è£å‰ªåçš„å›¾åƒï¼Œå›ç­”ï¼š{sample['question']}"
        refined_answer = call_qwen(prompt2, config.TEMP_EVIDENCE_PATH, config,
                                   cache_manager, use_cache=not warmup_mode)
        result.qwen_calls += 1

        if not refined_answer:
            result.notes = f"Qwené‡ç­”å¤±è´¥ (è¿­ä»£{iteration})"
            break

        print(f"ğŸ“¥ Qwenç²¾ç‚¼å›ç­”: {refined_answer}")

        # CLIPéªŒè¯ï¼ˆå¸¦ç¼“å­˜ï¼‰
        confidence = call_clip(evidence_bytes, refined_answer, config, cache_manager)
        result.clip_calls += 1
        result.clip_scores[f"iteration_{iteration}"] = float(confidence)

        print(f"ğŸ¯ CLIPç½®ä¿¡åº¦: {confidence:.3f} (é˜ˆå€¼: {config.CONFIDENCE_THRESHOLD})")

        if confidence >= config.CONFIDENCE_THRESHOLD:
            result.refined_answer = refined_answer
            result.final_confidence = float(confidence)
            print(f"âœ… éªŒè¯é€šè¿‡!")
            break
        elif retry == 0:
            # ç¬¬ä¸€æ¬¡éªŒè¯å¤±è´¥ï¼Œå°è¯•å…¨å›¾
            print("âš ï¸ ç¬¬ä¸€æ¬¡éªŒè¯å¤±è´¥ï¼Œå°è¯•å…¨å›¾...")
            bbox_str = f"0,0,{img_w},{img_h}"
        else:
            print(f"âš ï¸ ç¬¬{retry + 1}æ¬¡éªŒè¯å¤±è´¥")

    result.iteration_count = iteration
    result.total_time = time.time() - start_time

    # å¦‚æœç²¾ç‚¼ç­”æ¡ˆä¸ºç©ºï¼Œä½¿ç”¨åˆå§‹ç­”æ¡ˆ
    if not result.refined_answer and result.initial_answer:
        result.refined_answer = result.initial_answer
        # å¦‚æœæ²¡æœ‰CLIPéªŒè¯ï¼Œä½¿ç”¨é»˜è®¤ç½®ä¿¡åº¦
        if result.final_confidence == 0.0:
            result.final_confidence = 0.5  # é»˜è®¤ä¸­ç­‰ç½®ä¿¡åº¦

    # è¯„ä¼°å‡†ç¡®æ€§
    answer_to_evaluate = result.refined_answer if result.refined_answer else result.initial_answer
    result.accuracy, result.is_correct = calculate_accuracy(
        answer_to_evaluate,
        sample['answers']
    )

    # åˆ†æå¤±è´¥ç±»å‹
    if not result.is_correct:
        result.failure_type = analyze_failure_type(result, config)
        print(f"âŒ ç­”æ¡ˆé”™è¯¯ï¼Œå¤±è´¥ç±»å‹: {result.failure_type}")
    else:
        print(f"âœ… ç­”æ¡ˆæ­£ç¡®!")

    print(f"â±ï¸ å¤„ç†æ—¶é—´: {result.total_time:.2f}ç§’")
    print(f"ğŸ”„ è¿­ä»£æ¬¡æ•°: {result.iteration_count}")

    # ä¿å­˜å®Œæ•´ç»“æœåˆ°ç¼“å­˜ï¼ˆéé¢„çƒ­æ¨¡å¼ï¼‰
    if cache_manager and not warmup_mode:
        full_result = {
            'initial_answer': result.initial_answer,
            'initial_bbox': result.initial_bbox,
            'refined_answer': result.refined_answer,
            'final_confidence': result.final_confidence,
            'iteration_count': result.iteration_count,
            'is_correct': result.is_correct,
            'accuracy': result.accuracy
        }
        cache_manager.save_result_cache(image_path, sample['question'], {
            'full_result': full_result,
            'timestamp': time.time()
        })

    return result


def save_sam_segment(segment_data: bytes, original_image_path: str,
                     bbox_str: str, iteration: int, config: Config):
    """ä¿å­˜SAMåˆ†å‰²çš„å›¾åƒ"""
    # åˆ›å»ºç›®å½•
    os.makedirs(config.SAM_SEGMENTS_DIR, exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶å
    base_name = os.path.splitext(os.path.basename(original_image_path))[0]
    if iteration == 1:
        suffix = "initial"
    elif iteration == 2:
        suffix = "full"
    else:
        suffix = f"retry{iteration}"

    # ç®€åŒ–bboxå­—ç¬¦ä¸²ç”¨äºæ–‡ä»¶åï¼ˆç§»é™¤é€—å·ï¼‰
    bbox_simple = bbox_str.replace(',', '_')

    # å®Œæ•´çš„æ–‡ä»¶å
    filename = f"{base_name}_{suffix}_{bbox_simple}.png"
    filepath = os.path.join(config.SAM_SEGMENTS_DIR, filename)

    # ä¿å­˜æ–‡ä»¶
    with open(filepath, "wb") as f:
        f.write(segment_data)

    return filepath


# ==================== é¢„çƒ­ç¼“å­˜ ====================
def warmup_cache(samples: List[Dict], config: Config, cache_manager: CacheManager):
    """é¢„çƒ­ç¼“å­˜ï¼šé¢„å…ˆå¤„ç†ä¸€äº›æ ·æœ¬å¡«å……ç¼“å­˜"""
    if not config.ENABLE_CACHE or config.CACHE_WARMUP_SIZE <= 0:
        return

    print(f"\nğŸ”¥ å¼€å§‹é¢„çƒ­ç¼“å­˜ï¼Œæ ·æœ¬æ•°: {min(config.CACHE_WARMUP_SIZE, len(samples))}")

    warmup_samples = samples[:min(config.CACHE_WARMUP_SIZE, len(samples))]

    for i, sample in enumerate(tqdm(warmup_samples, desc="é¢„çƒ­ç¼“å­˜")):
        print(f"\né¢„çƒ­æ ·æœ¬ {i + 1}/{len(warmup_samples)}")
        result = run_single_experiment(sample, config, cache_manager, warmup_mode=True)

        # æ ‡è®°ä¸ºé¢„çƒ­å‘½ä¸­
        if cache_manager:
            cache_manager.cache_stats["warmup_hits"] += 1

    print(f"âœ… ç¼“å­˜é¢„çƒ­å®Œæˆ")


# ==================== å®éªŒç®¡ç†ï¼ˆå¸¦ç¼“å­˜ï¼‰ ====================
class ExperimentManager:
    def __init__(self, config: Config):
        self.config = config
        self.results: List[ExperimentResult] = []
        self.stats = SystemStatistics()
        self.cache_manager = CacheManager(config) if config.ENABLE_CACHE else None

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(config.OUTPUT_DIR, exist_ok=True)
        os.makedirs(config.SAM_SEGMENTS_DIR, exist_ok=True)

        print(f"ğŸ“ è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}")
        if config.ENABLE_CACHE:
            print(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {config.CACHE_DIR}")

    def run_experiments(self):
        """è¿è¡Œæ‰€æœ‰å®éªŒï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        print("ğŸš€ å¼€å§‹å¸¦ç¼“å­˜çš„VQAé—­ç¯ç³»ç»Ÿå®éªŒ...")

        # åŠ è½½æ•°æ®
        samples = load_textvqa_dataset(self.config)
        self.stats.total_samples = len(samples)

        # é¢„çƒ­ç¼“å­˜
        if self.config.ENABLE_CACHE:
            warmup_cache(samples, self.config, self.cache_manager)

        # é€ä¸ªè¿è¡Œå®éªŒ
        for i, sample in enumerate(tqdm(samples, desc="è¿›è¡Œå®éªŒ")):
            print(f"\n{'=' * 60}")
            print(f"æ ·æœ¬ {i + 1}/{len(samples)}: {sample['question']}")
            print(f"å›¾åƒ: {sample['image_file']}")
            print(f"å‚è€ƒç­”æ¡ˆ: {sample['answers'][:3]}")

            result = run_single_experiment(sample, self.config, self.cache_manager)
            self.results.append(result)

            # æ›´æ–°ç»Ÿè®¡
            self.stats.correct_samples += 1 if result.is_correct else 0
            self.stats.total_iterations += result.iteration_count
            self.stats.total_sam_calls += result.sam_calls
            self.stats.total_clip_calls += result.clip_calls
            self.stats.total_qwen_calls += result.qwen_calls
            self.stats.total_time += result.total_time

            if result.failure_type:
                self.stats.failure_counts[result.failure_type] += 1

            # æ¯5ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 5 == 0:
                self.save_results()
                print(f"\nğŸ’¾ å·²ä¿å­˜{len(self.results)}ä¸ªæ ·æœ¬çš„ç»“æœ")

        # è®¡ç®—ç¼“å­˜ç»Ÿè®¡å’ŒåŠ é€Ÿæ¯”
        self._calculate_cache_stats()

        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.save_results()
        self.generate_report()
        print("\nâœ… å¸¦ç¼“å­˜çš„å®éªŒå®Œæˆ!")

    def _calculate_cache_stats(self):
        """è®¡ç®—ç¼“å­˜ç»Ÿè®¡å’ŒåŠ é€Ÿæ¯”"""
        if not self.config.ENABLE_CACHE or not self.cache_manager:
            return

        # è·å–ç¼“å­˜ç»Ÿè®¡
        self.stats.cache_stats = self.cache_manager.get_cache_stats()

        # ä¼°è®¡åŠ é€Ÿæ¯”
        # å‡è®¾æ¯æ¬¡ç½‘ç»œè°ƒç”¨å¹³å‡è€—æ—¶ï¼šQwen=2s, SAM=1s, CLIP=0.5s
        avg_qwen_time = 2.0
        avg_sam_time = 1.0
        avg_clip_time = 0.5

        # è®¡ç®—èŠ‚çœçš„ç½‘ç»œè°ƒç”¨
        qwen_hits = self.stats.cache_stats.get('result_hits', 0)
        sam_hits = self.stats.cache_stats.get('sam_hits', 0)
        clip_hits = self.stats.cache_stats.get('text_hits', 0)

        time_saved = (qwen_hits * avg_qwen_time +
                      sam_hits * avg_sam_time +
                      clip_hits * avg_clip_time)

        # åŠ é€Ÿæ¯” = æ€»æ—¶é—´ / (æ€»æ—¶é—´ - èŠ‚çœæ—¶é—´)
        if self.stats.total_time > 0 and time_saved > 0:
            self.stats.estimated_speedup = self.stats.total_time / (self.stats.total_time - time_saved)

        # ç¼“å­˜æ”¶ç›Šæ¯” = èŠ‚çœæ—¶é—´ / æ€»æ—¶é—´
        if self.stats.total_time > 0:
            self.stats.cache_benefit_ratio = time_saved / self.stats.total_time

        # æ‰“å°ç¼“å­˜ç»Ÿè®¡
        self.cache_manager.print_cache_stats()

        print(f"\nâš¡ åŠ é€Ÿæ¯”åˆ†æ:")
        print(f"   ä¼°è®¡èŠ‚çœæ—¶é—´: {time_saved:.2f}ç§’")
        print(f"   ä¼°è®¡åŠ é€Ÿæ¯”: {self.stats.estimated_speedup:.2f}x")
        print(f"   ç¼“å­˜æ”¶ç›Šæ¯”: {self.stats.cache_benefit_ratio:.1%}")

    def save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        # è½¬æ¢ç»“æœä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        results_list = []
        for r in self.results:
            result_dict = {
                'id': int(r.sample_id),
                'question': str(r.question),
                'image_file': str(r.image_file),
                'ground_truth': [str(ans) for ans in r.ground_truth_answers],
                'initial_answer': str(r.initial_answer),
                'initial_bbox': str(r.initial_bbox),
                'refined_answer': str(r.refined_answer),
                'confidence': float(r.final_confidence),
                'clip_scores': {k: float(v) for k, v in r.clip_scores.items()},
                'is_correct': bool(r.is_correct),
                'accuracy': float(r.accuracy),
                'failure_type': str(r.failure_type),
                'iteration_count': int(r.iteration_count),
                'sam_calls': int(r.sam_calls),
                'clip_calls': int(r.clip_calls),
                'qwen_calls': int(r.qwen_calls),
                'time': float(r.total_time),
                'from_cache': bool(r.from_cache),
                'cache_hits': r.cache_hits,
                'notes': str(r.notes)
            }
            results_list.append(result_dict)

        results_dict = {
            'config': {
                'max_retries': int(self.config.MAX_RETRIES),
                'confidence_threshold': float(self.config.CONFIDENCE_THRESHOLD),
                'enable_cache': bool(self.config.ENABLE_CACHE),
                'cache_warmup_size': int(self.config.CACHE_WARMUP_SIZE),
                'batch_size': int(self.config.BATCH_SIZE),
                'num_samples': int(self.config.NUM_SAMPLES),
                'random_seed': int(self.config.RANDOM_SEED)
            },
            'statistics': {
                'total_samples': int(self.stats.total_samples),
                'correct_samples': int(self.stats.correct_samples),
                'accuracy': float(self.stats.accuracy),
                'total_iterations': int(self.stats.total_iterations),
                'avg_iterations': float(self.stats.avg_iterations),
                'total_sam_calls': int(self.stats.total_sam_calls),
                'total_clip_calls': int(self.stats.total_clip_calls),
                'total_qwen_calls': int(self.stats.total_qwen_calls),
                'total_time': float(self.stats.total_time),
                'avg_time_per_sample': float(self.stats.avg_time_per_sample),
                'estimated_speedup': float(self.stats.estimated_speedup),
                'cache_benefit_ratio': float(self.stats.cache_benefit_ratio),
                'failure_counts': {k: int(v) for k, v in self.stats.failure_counts.items()}
            },
            'cache_statistics': self.stats.cache_stats if self.stats.cache_stats else {},
            'results': results_list
        }

        # ä¿å­˜JSONæ ¼å¼çš„è¯¦ç»†ç»“æœ
        with open(self.config.RESULTS_JSON, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2, default=str)

        # ä¿å­˜CSVæ ¼å¼çš„ç»Ÿè®¡ä¿¡æ¯
        with open(self.config.STATS_CSV, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'æ ·æœ¬ID', 'é—®é¢˜', 'å›¾åƒ', 'å‚è€ƒç­”æ¡ˆ',
                'åˆå§‹ç­”æ¡ˆ', 'ç²¾ç‚¼ç­”æ¡ˆ', 'ç½®ä¿¡åº¦',
                'æ˜¯å¦æ­£ç¡®', 'å‡†ç¡®ç‡', 'å¤±è´¥ç±»å‹',
                'è¿­ä»£æ¬¡æ•°', 'SAMè°ƒç”¨', 'CLIPè°ƒç”¨', 'Qwenè°ƒç”¨',
                'æ—¶é—´(s)', 'æ¥è‡ªç¼“å­˜', 'ç¼“å­˜å‘½ä¸­', 'å¤‡æ³¨'
            ])

            for r in self.results:
                cache_hits_str = ';'.join([f"{k}:{v}" for k, v in r.cache_hits.items()])
                writer.writerow([
                    int(r.sample_id),
                    str(r.question)[:50],
                    str(r.image_file),
                    '; '.join([str(ans) for ans in r.ground_truth_answers[:3]]),
                    str(r.initial_answer)[:30],
                    str(r.refined_answer)[:30],
                    f"{float(r.final_confidence):.3f}",
                    "æ˜¯" if r.is_correct else "å¦",
                    f"{float(r.accuracy):.3f}",
                    str(r.failure_type),
                    int(r.iteration_count),
                    int(r.sam_calls),
                    int(r.clip_calls),
                    int(r.qwen_calls),
                    f"{float(r.total_time):.2f}",
                    "æ˜¯" if r.from_cache else "å¦",
                    cache_hits_str,
                    str(r.notes)[:50]
                ])

        # ä¿å­˜ç¼“å­˜ç»Ÿè®¡
        if self.stats.cache_stats:
            with open(self.config.CACHE_STATS_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.stats.cache_stats, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜è‡³: {self.config.OUTPUT_DIR}")

    def generate_report(self):
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        report = f"""
# å¸¦ç¼“å­˜ç­–ç•¥çš„VQAé—­ç¯ç³»ç»Ÿå®éªŒæŠ¥å‘Š

## 1. å®éªŒæ¦‚è¿°
- **ç³»ç»Ÿç±»å‹**: å¸¦å¤šçº§ç¼“å­˜çš„VQAé—­ç¯ç³»ç»Ÿ
- **æ•°æ®é›†**: MyVQAï¼ˆ{self.stats.total_samples}ä¸ªæ ·æœ¬ï¼‰
- **é—­ç¯é…ç½®**: æœ€å¤§è¿­ä»£{self.config.MAX_RETRIES}æ¬¡ï¼Œç½®ä¿¡åº¦é˜ˆå€¼{self.config.CONFIDENCE_THRESHOLD}
- **ç¼“å­˜é…ç½®**: å¯ç”¨ç¼“å­˜={self.config.ENABLE_CACHE}ï¼Œé¢„çƒ­å¤§å°={self.config.CACHE_WARMUP_SIZE}ï¼Œæ‰¹é‡å¤§å°={self.config.BATCH_SIZE}
- **éšæœºç§å­**: {self.config.RANDOM_SEED}

## 2. ä¸»è¦ç»“æœ
- **æ€»ä½“å‡†ç¡®ç‡**: {self.stats.accuracy:.2%} ({self.stats.correct_samples}/{self.stats.total_samples})
- **å¹³å‡è¿­ä»£æ¬¡æ•°**: {self.stats.avg_iterations:.2f}
- **å¹³å‡å¤„ç†æ—¶é—´**: {self.stats.avg_time_per_sample:.2f}ç§’/æ ·æœ¬
- **æ€»å®éªŒæ—¶é—´**: {self.stats.total_time:.2f}ç§’

## 3. ç¼“å­˜æ€§èƒ½åˆ†æ
"""

        if self.config.ENABLE_CACHE:
            report += f"""
### 3.1 ç¼“å­˜å‘½ä¸­ç‡
- **æ€»ä½“å‘½ä¸­ç‡**: {self.stats.cache_stats.get('hit_rate', 0) * 100:.1f}%
- **å›¾åƒç¼“å­˜å‘½ä¸­**: {self.stats.cache_stats.get('image_hits', 0)}æ¬¡
- **æ–‡æœ¬ç¼“å­˜å‘½ä¸­**: {self.stats.cache_stats.get('text_hits', 0)}æ¬¡
- **SAMç¼“å­˜å‘½ä¸­**: {self.stats.cache_stats.get('sam_hits', 0)}æ¬¡
- **ç»“æœç¼“å­˜å‘½ä¸­**: {self.stats.cache_stats.get('result_hits', 0)}æ¬¡
- **ç›¸ä¼¼æ€§ç¼“å­˜å‘½ä¸­**: {self.stats.cache_stats.get('similarity_hits', 0)}æ¬¡
- **é¢„çƒ­ç¼“å­˜å‘½ä¸­**: {self.stats.cache_stats.get('warmup_hits', 0)}æ¬¡

### 3.2 åŠ é€Ÿæ•ˆæœ
- **ä¼°è®¡åŠ é€Ÿæ¯”**: {self.stats.estimated_speedup:.2f}x
- **ç¼“å­˜æ”¶ç›Šæ¯”**: {self.stats.cache_benefit_ratio:.1%}
- **å‡å°‘çš„ç½‘ç»œè°ƒç”¨**: 
  * Qwenè°ƒç”¨å‡å°‘: {self.stats.cache_stats.get('result_hits', 0)}æ¬¡
  * SAMè°ƒç”¨å‡å°‘: {self.stats.cache_stats.get('sam_hits', 0)}æ¬¡
  * CLIPè°ƒç”¨å‡å°‘: {self.stats.cache_stats.get('text_hits', 0)}æ¬¡
"""
        else:
            report += "- **ç¼“å­˜æœªå¯ç”¨**\n"

        report += """
## 4. å·¥å…·è°ƒç”¨ç»Ÿè®¡
- SAMè°ƒç”¨æ¬¡æ•°: {self.stats.total_sam_calls}
- CLIPè°ƒç”¨æ¬¡æ•°: {self.stats.total_clip_calls}
- Qwenè°ƒç”¨æ¬¡æ•°: {self.stats.total_qwen_calls}

## 5. å¤±è´¥åˆ†æ
"""

        total_failures = sum(self.stats.failure_counts.values())
        for failure_type, count in self.stats.failure_counts.items():
            if count > 0:
                percentage = count / total_failures * 100 if total_failures > 0 else 0
                report += f"- **{failure_type}**: {count}æ¬¡ ({percentage:.1f}%)\n"

        report += """
## 6. ç¼“å­˜ç­–ç•¥è®¾è®¡

### 6.1 å¤šçº§ç¼“å­˜æ¶æ„
1. **å›¾åƒç‰¹å¾ç¼“å­˜**: ç¼“å­˜å›¾åƒçš„Base64ç¼–ç ï¼Œé¿å…é‡å¤ç¼–ç 
2. **æ–‡æœ¬ç‰¹å¾ç¼“å­˜**: ç¼“å­˜æ–‡æœ¬çš„å“ˆå¸Œå’ŒCLIPç›¸ä¼¼åº¦åˆ†æ•°
3. **SAMç»“æœç¼“å­˜**: ç¼“å­˜ç›¸åŒå›¾åƒå’Œbboxçš„åˆ†å‰²ç»“æœ
4. **å®Œæ•´ç»“æœç¼“å­˜**: ç¼“å­˜æ•´ä¸ªå®éªŒæµç¨‹çš„ç»“æœ
5. **ç›¸ä¼¼æ€§ç¼“å­˜**: åŸºäºå›¾åƒå’Œé—®é¢˜çš„ç›¸ä¼¼æ€§æŸ¥æ‰¾ç¼“å­˜

### 6.2 ç¼“å­˜é”®è®¾è®¡
- **å›¾åƒç¼“å­˜é”®**: å›¾åƒæ„ŸçŸ¥å“ˆå¸Œï¼ˆPHashï¼‰
- **æ–‡æœ¬ç¼“å­˜é”®**: MD5å“ˆå¸Œ
- **SAMç¼“å­˜é”®**: å›¾åƒå“ˆå¸Œ + bboxå“ˆå¸Œ
- **ç»“æœç¼“å­˜é”®**: å›¾åƒå“ˆå¸Œ + é—®é¢˜å“ˆå¸Œ

### 6.3 é¢„çƒ­ç­–ç•¥
- é¢„å…ˆå¤„ç†éƒ¨åˆ†æ ·æœ¬å¡«å……ç¼“å­˜
- æé«˜åç»­è¯·æ±‚çš„ç¼“å­˜å‘½ä¸­ç‡

## 7. ä¸æ— ç¼“å­˜ç³»ç»Ÿå¯¹æ¯”ä¼˜åŠ¿
1. **æ˜¾è‘—å‡å°‘ç½‘ç»œå»¶è¿Ÿ**: ç¼“å­˜å‘½ä¸­æ—¶è·³è¿‡ç½‘ç»œè¯·æ±‚
2. **é™ä½æœåŠ¡å™¨è´Ÿè½½**: å‡å°‘å¯¹åç«¯æœåŠ¡çš„é‡å¤è°ƒç”¨
3. **æé«˜ç³»ç»Ÿå“åº”é€Ÿåº¦**: æœ¬åœ°ç¼“å­˜è®¿é—®é€Ÿåº¦å¿«äºç½‘ç»œè¯·æ±‚
4. **æ”¯æŒç¦»çº¿å›æ”¾**: ç¼“å­˜ç»“æœå¯ç”¨äºç¦»çº¿åˆ†æå’Œè°ƒè¯•

## 8. ç¼“å­˜å¼€é”€åˆ†æ
1. **å­˜å‚¨å¼€é”€**: éœ€è¦ç£ç›˜ç©ºé—´å­˜å‚¨ç¼“å­˜æ–‡ä»¶
2. **å†…å­˜å¼€é”€**: å†…å­˜ç¼“å­˜å ç”¨ä¸€å®šRAM
3. **ä¸€è‡´æ€§å¼€é”€**: éœ€è¦å¤„ç†ç¼“å­˜å¤±æ•ˆå’Œæ›´æ–°

## 9. ä¼˜åŒ–å»ºè®®
1. **æ™ºèƒ½ç¼“å­˜æ·˜æ±°**: å®ç°LRUæˆ–LFUç¼“å­˜æ·˜æ±°ç­–ç•¥
2. **å¢é‡æ›´æ–°**: åªç¼“å­˜å˜åŒ–çš„éƒ¨åˆ†ï¼Œå‡å°‘å­˜å‚¨å¼€é”€
3. **åˆ†å¸ƒå¼ç¼“å­˜**: åœ¨å¤šæœºéƒ¨ç½²æ—¶ä½¿ç”¨Redisç­‰åˆ†å¸ƒå¼ç¼“å­˜
4. **é¢„æµ‹æ€§é¢„çƒ­**: åŸºäºå†å²è®¿é—®æ¨¡å¼é¢„æµ‹æ€§é¢„çƒ­ç¼“å­˜

## 10. æ ·æœ¬ç¤ºä¾‹
"""

        # æ·»åŠ 3ä¸ªç¤ºä¾‹ç»“æœ
        for i, r in enumerate(self.results[:3]):
            cache_source = "æ¥è‡ªç¼“å­˜" if r.from_cache else "å®æ—¶è®¡ç®—"
            report += f"""
### ç¤ºä¾‹ {i + 1}
- **æ ·æœ¬ID**: {r.sample_id}
- **é—®é¢˜**: {r.question}
- **å›¾åƒ**: {r.image_file}
- **å¤„ç†æ–¹å¼**: {cache_source}
- **ç²¾ç‚¼ç­”æ¡ˆ**: {r.refined_answer}
- **CLIPç½®ä¿¡åº¦**: {r.final_confidence:.3f}
- **æ˜¯å¦æ­£ç¡®**: {'æ˜¯' if r.is_correct else 'å¦'}
- **å¤„ç†æ—¶é—´**: {r.total_time:.2f}ç§’
- **ç¼“å­˜å‘½ä¸­**: {sum(r.cache_hits.values())}æ¬¡
"""

        report_path = os.path.join(self.config.OUTPUT_DIR, "experiment_report_with_cache.md")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"ğŸ“Š æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")


# ==================== ä¸»ç¨‹åº ====================
def main():
    # åˆå§‹åŒ–é…ç½®
    config = Config()

    # ç¡®ä¿æ‰€æœ‰è¾“å‡ºç›®å½•éƒ½å­˜åœ¨
    os.makedirs(config.OUTPUT_DIR, exist_ok=True)
    os.makedirs(config.SAM_SEGMENTS_DIR, exist_ok=True)
    if config.ENABLE_CACHE:
        os.makedirs(config.CACHE_DIR, exist_ok=True)

    # è¿è¡Œä¸»å®éªŒ
    print("=" * 80)
    print("ğŸš€ å¸¦ç¼“å­˜ç­–ç•¥çš„VQAé—­ç¯ç³»ç»Ÿå®éªŒ")
    print(f"ğŸ’¾ ç¼“å­˜å¯ç”¨: {config.ENABLE_CACHE}")
    print(f"ğŸ”¥ é¢„çƒ­å¤§å°: {config.CACHE_WARMUP_SIZE}")
    print(f"âš¡ æ‰¹é‡å¤§å°: {config.BATCH_SIZE}")
    print("=" * 80)

    manager = ExperimentManager(config)
    manager.run_experiments()

    print(f"\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {config.OUTPUT_DIR}")
    print(f"ğŸ“„ è¯¦ç»†ç»“æœ: {config.RESULTS_JSON}")
    print(f"ğŸ“Š ç»Ÿè®¡è¡¨æ ¼: {config.STATS_CSV}")
    print(f"ğŸ’¾ ç¼“å­˜ç»Ÿè®¡: {config.CACHE_STATS_FILE}")
    print(f"ğŸ“‹ å®éªŒæŠ¥å‘Š: {config.OUTPUT_DIR}/experiment_report_with_cache.md")

    if config.ENABLE_CACHE:
        print(f"\nğŸ“Š ç¼“å­˜ç›®å½•ç»“æ„:")
        print(f"   å›¾åƒç¼“å­˜: {config.IMAGE_CACHE_DIR}")
        print(f"   æ–‡æœ¬ç¼“å­˜: {config.TEXT_CACHE_DIR}")
        print(f"   SAMç¼“å­˜: {config.SAM_CACHE_DIR}")
        print(f"   ç»“æœç¼“å­˜: {config.RESULT_CACHE_DIR}")


if __name__ == "__main__":
    main()
