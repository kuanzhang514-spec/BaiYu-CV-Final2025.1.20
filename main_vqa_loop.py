'''
æµ‹è¯•ä¸€ä¸‹æ–¹æ¡ˆè¡Œä¸è¡Œ

'''

import requests
import base64
import os
from PIL import Image
from io import BytesIO
import json
import time

# ===== é…ç½® =====
SERVER_IP = "è¿™æ˜¯æˆ‘çš„æœåŠ¡å™¨IPåœ°å€ï¼Œæˆ‘éšè—äº†"
QWEN_URL = f"http://{SERVER_IP}:8020/chat_vl"
CLIP_URL = f"http://{SERVER_IP}:8021/clip/score"
SAM_URL = f"http://{SERVER_IP}:8022/sam/segment"

# æœ¬åœ°æµ‹è¯•å›¾
IMAGE_PATH = r"C:\Users\kuanzhang\Desktop\courseB\fuwuqisanhaoji\Uploadimg\pingguo.jpg"
QUESTION = "What color is the apple?"

# å·¥å…·è°ƒç”¨è®¡æ•°å™¨
stats = {
    "qwen_calls": 0,
    "sam_calls": 0,
    "clip_calls": 0,
    "iterations": 0
}

def image_to_base64(image_path, max_size=(512, 512)):
    """å°†å›¾åƒè½¬ä¸º base64ï¼ˆç”¨äº Qwen3-VLï¼‰"""
    img = Image.open(image_path)
    if img.mode in ("RGBA", "P"):
        img = img.convert("RGB")
    img.thumbnail(max_size)
    buffered = BytesIO()
    img.save(buffered, format="JPEG", quality=85)
    return base64.b64encode(buffered.getvalue()).decode('utf-8')

def call_qwen(prompt, image_path=None):
    """è°ƒç”¨ Qwen3-VL"""
    stats["qwen_calls"] += 1
    payload = {"prompt": prompt}
    if image_path:
        payload["image_url"] = image_to_base64(image_path)
    try:
        response = requests.post(QWEN_URL, json=payload, timeout=60)
        if response.status_code == 200:
            return response.json().get("response", "").strip()
        else:
            print(f"âŒ Qwen error: {response.status_code}")
            return ""
    except Exception as e:
        print(f"ğŸ’¥ Qwen exception: {e}")
        return ""

def call_sam(original_img_path, bbox_str):
    """è°ƒç”¨ SAMï¼Œè¿”å›è£å‰ªå›¾çš„ bytes"""
    stats["sam_calls"] += 1
    with open(original_img_path, 'rb') as f:
        files = {'imagefile': ('image.jpg', f, 'image/jpeg')}
        data = {'bbox': bbox_str}
        response = requests.post(SAM_URL, files=files, data=data, timeout=30)
        if response.status_code == 200:
            return response.content  # PNG bytes
        else:
            print(f"âŒ SAM error: {response.text}")
            return None

def call_clip(image_bytes, text):
    """è°ƒç”¨ CLIPï¼Œè¿”å›ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆå‡è®¾ text æ˜¯å”¯ä¸€æ ‡ç­¾ï¼‰"""
    stats["clip_calls"] += 1
    files = {'imagefile': ('evidence.png', image_bytes, 'image/png')}
    data = {'text': text}
    try:
        response = requests.post(CLIP_URL, files=files, data=data, timeout=10)
        if response.status_code == 200:
            res = response.json()
            # ä½ çš„ CLIP è¿”å› results: {"a photo of cat": {"similarity": 0.8}, ...}
            # æˆ‘ä»¬å–ç¬¬ä¸€ä¸ªï¼ˆæˆ–åŒ¹é… text çš„ï¼‰
            for label, val in res['results'].items():
                if text.lower() in label.lower() or label.lower() in text.lower():
                    return float(val['similarity'])
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ª
            first_key = list(res['results'].keys())[0]
            return float(res['results'][first_key]['similarity'])
        else:
            print(f"âŒ CLIP error: {response.text}")
            return 0.0
    except Exception as e:
        print(f"ğŸ’¥ CLIP exception: {e}")
        return 0.0

def extract_bbox_from_description(desc, img_w=1000, img_h=1000):
    """
    ç®€åŒ–ç‰ˆï¼šä»æè¿°ä¸­æå– bboxï¼ˆå®é™…é¡¹ç›®å¯ç”¨ CLIP ç½‘æ ¼å®šä½ï¼‰
    è¿™é‡Œæˆ‘ä»¬ç”¨å¯å‘å¼è§„åˆ™ï¼ˆä»…ä½œæ¼”ç¤ºï¼‰ï¼š
    - å¦‚æœæåˆ° "left" â†’ x1=0, x2=img_w//2
    - å¦‚æœæåˆ° "right" â†’ x1=img_w//2, x2=img_w
    - é»˜è®¤æ•´ä¸ªå›¾
    """
    desc = desc.lower()
    if "left" in desc:
        return f"0,0,{img_w//2},{img_h}"
    elif "right" in desc:
        return f"{img_w//2},0,{img_w},{img_h}"
    elif "top" in desc:
        return f"0,0,{img_w},{img_h//2}"
    elif "bottom" in desc:
        return f"0,{img_h//2},{img_w},{img_h}"
    else:
        # é»˜è®¤ä¸­å¿ƒåŒºåŸŸï¼ˆå¯æ ¹æ®éœ€æ±‚è°ƒæ•´ï¼‰
        cx, cy = img_w // 2, img_h // 2
        size = min(img_w, img_h) // 2
        x1, y1 = cx - size//2, cy - size//2
        x2, y2 = cx + size//2, cy + size//2
        return f"{x1},{y1},{x2},{y2}"

def main():
    print(f"ğŸš€ å¼€å§‹ VQA é—­ç¯ä»»åŠ¡")
    print(f"ğŸ–¼ï¸ å›¾åƒ: {os.path.basename(IMAGE_PATH)}")
    print(f"â“ é—®é¢˜: {QUESTION}\n")

    # Step 1: åˆå§‹æé—®ï¼Œè·å–ç­”æ¡ˆ + è¯æ®æè¿°
    prompt1 = f"Question: {QUESTION}. First, describe what visual region you need to see to answer this question. Then give your answer."
    response1 = call_qwen(prompt1, IMAGE_PATH)
    print(f"ğŸ§  Qwen åˆæ­¥å›ç­”:\n{response1}\n")

    # ç®€å•è§£æï¼šå‡è®¾æœ€åä¸€å¥æ˜¯ç­”æ¡ˆï¼Œå‰é¢æ˜¯æè¿°
    lines = response1.split(". ")
    evidence_desc = ". ".join(lines[:-1]) + "."
    initial_answer = lines[-1].strip()

    # Step 2: è·å–åŸå§‹å›¾åƒå°ºå¯¸ï¼ˆç”¨äº bboxï¼‰
    with Image.open(IMAGE_PATH) as img:
        img_w, img_h = img.size

    # Step 3: æå– bboxï¼ˆå®é™…å¯ç”¨ CLIP å®šä½ï¼Œæ­¤å¤„ç®€åŒ–ï¼‰
    bbox = extract_bbox_from_description(evidence_desc, img_w, img_h)
    print(f"ğŸ“ æå– BBox: {bbox}")

    # Step 4: è°ƒç”¨ SAM è·å–è¯æ®å›¾
    evidence_img_bytes = call_sam(IMAGE_PATH, bbox)
    if not evidence_img_bytes:
        print("ğŸ›‘ SAM å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
        return

    # ä¿å­˜è¯æ®å›¾ï¼ˆå¯é€‰ï¼‰
    with open("evidence_step1.png", "wb") as f:
        f.write(evidence_img_bytes)

    # Step 5: åŸºäºè¯æ®å›¾å†æ¬¡æé—®
    prompt2 = f"Based ONLY on this image, answer: {QUESTION}"
    # å°† evidence_img_bytes è½¬ä¸ºä¸´æ—¶è·¯å¾„ä¾› Qwen ä½¿ç”¨
    evidence_temp_path = "temp_evidence.jpg"
    with Image.open(BytesIO(evidence_img_bytes)) as evidence_img:
        evidence_img.convert("RGB").save(evidence_temp_path, "JPEG")

    refined_answer = call_qwen(prompt2, evidence_temp_path)
    print(f"ğŸ¯ åŸºäºè¯æ®çš„å›ç­”: {refined_answer}")

    # Step 6: ç”¨ CLIP éªŒè¯
    verification_text = f"The answer is {refined_answer}."
    clip_score = call_clip(evidence_img_bytes, verification_text)
    print(f"ğŸ” CLIP éªŒè¯åˆ†æ•°: {clip_score:.3f}")

    # Step 7: å†³ç­–
    CONFIDENCE_THRESHOLD = 0.7
    final_answer = refined_answer
    stats["iterations"] = 1

    if clip_score < CONFIDENCE_THRESHOLD:
        print("âš ï¸ ç½®ä¿¡åº¦ä½ï¼Œå°è¯•æ‰©å¤§åŒºåŸŸé‡è¯•...")
        # æ‰©å¤§ bboxï¼ˆä¾‹å¦‚å…¨å›¾ï¼‰
        full_bbox = f"0,0,{img_w},{img_h}"
        evidence_img_bytes2 = call_sam(IMAGE_PATH, full_bbox)
        if evidence_img_bytes2:
            with open("evidence_step2.png", "wb") as f:
                f.write(evidence_img_bytes2)
                print(f"ğŸ“ temp_evidence2.jpg å¤§å°: {len(f.read())} bytes")
            with Image.open(BytesIO(evidence_img_bytes2)) as img2:
                img2.convert("RGB").save("temp_evidence2.jpg", "JPEG")
            refined_answer2 = call_qwen(prompt2, "temp_evidence2.jpg")
            print(f"ğŸ” ç¬¬äºŒæ¬¡å›ç­”: {refined_answer2}")
            verification_text2 = f"The answer is {refined_answer2}."
            clip_score2 = call_clip(evidence_img_bytes2, verification_text2)
            print(f"ğŸ”„ ç¬¬äºŒæ¬¡ CLIP åˆ†æ•°: {clip_score2:.3f}")
            stats["iterations"] = 2
            if clip_score2 > clip_score:
                final_answer = refined_answer2
                clip_score = clip_score2

    # æœ€ç»ˆè¾“å‡º
    print("\n" + "="*50)
    print(f"âœ… æœ€ç»ˆç­”æ¡ˆ: {final_answer}")
    print(f"ğŸ“Š ç½®ä¿¡åº¦: {clip_score:.3f}")
    print(f"ğŸ“ˆ ç»Ÿè®¡: {stats}")
    print("="*50)

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    for f in ["temp_evidence.jpg", "temp_evidence2.jpg"]:
        if os.path.exists(f):
            os.remove(f)

if __name__ == "__main__":
    start_time = time.time()
    main()
    print(f"\nâ±ï¸ æ€»è€—æ—¶: {time.time() - start_time:.2f} ç§’")